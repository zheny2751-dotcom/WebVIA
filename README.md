# WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation

</div>

![image](./assets/WEBVIA.png)

<p align="center">
   ğŸ“„ <a href="https://arxiv.org/abs/XXX" target="_blank">Paper</a> &nbsp; â€¢ &nbsp;
   ğŸ¤— <a href="https://huggingface.co/your_model_link" target="_blank">Model</a> &nbsp; â€¢ &nbsp;
   ğŸŒ <a href="https://zheny2751-dotcom.github.io/webvia.github.io/" target="_blank">Demo</a>
</p>


---

## ğŸ§­ Overview

**WEBVIA** â€” the first agentic framework that supports **interactive** UI-to-Code generation and verification.

---

## ğŸ“‚ Repository Structure

WEBVIA-FOR-PUBLISH/
â”œâ”€â”€ scripts/                               # Source code of the full WEBVIA pipeline  
â”‚   â”œâ”€â”€ agent/                             # WEBVIA-Agent module code  
â”‚   â”‚   â””â”€â”€ start_agent.py                 # Agent launcher for all models  
â”‚   â”‚  
â”‚   â”œâ”€â”€ ui2code/                           # WEBVIA-UI2Code module code  
â”‚   â””â”€â”€ verificatio/                       # WEBVIA-Verification module code  
â”‚  
â”œâ”€â”€ evaluation/                            # Datasets and experiment scripts  
â”‚   â”œâ”€â”€ agent/  
â”‚   â””â”€â”€ ui2code/  
â”‚  
â”œâ”€â”€ environment/                           # Environment setup and dependencies  
â”‚   â””â”€â”€ webenv-init/  
â”‚  
â”œâ”€â”€ example/                               # Quick start examples  
â”‚   â”œâ”€â”€ agent/  
â”‚   â”œâ”€â”€ ui2code/  
â”‚   â””â”€â”€ verification/  
â”‚  
â””â”€â”€ README.md  

---

## âš™ï¸ Environment Setup

Run the following commands:

```bash
conda create -n webvia python=3.10
conda activate webvia
pip install -r requirements.txt
```

---

# ğŸš€ WEBVIA Pipeline Quick Start

**Note:** Before launching each module, fill in the OpenAI API configuration fields (`api_key` and `api_base`) in the corresponding config file.

**Note:** To ensure successful execution, run each module sequentially, as the output of one module serves as the input to the next in the WEBVIA pipeline.
---

## ğŸª„ End-to-End Workflow Overview

| Stage               | Input                 | Output                         | Next Stage   |
| ------------------- | --------------------- | ------------------------------ | ------------ |
| WEBVIA-Agent        | Raw HTML directory    | Exploration screenshots & logs | UI2Code      |
| WEBVIA-UI2Code      | Screenshots & logs    | Generated HTML files           | Verification |
| WEBVIA-Verification | Logs + Generated HTML | Verification results           | Done         |

---

## WEBVIA â€” Agent Module

**Description**
The Agent subsystem performs interactive exploration on webpages, recording screenshots and action logs.
It supports configuration through `--config` (relative or absolute path).


**Minimal Example**

1. Go to the example directory:

```bash
cd example/agent
```

2. Launch:

```bash
python start_agent.py --config config.json
```

This command reads the provided configuration and executes the full exploration process.

**Example Configuration (config.json):**

```json
{
  "input_type": "html",
  "input_html_dir": "./htmls",
  "input_url_txt": "./urls.txt",
  "image_dir": "./images",
  "bug_dir": "./error_htmls",
  "api_key": "sk-REPLACE_ME",
  "api_base": "https://api.openai.com/v1",
  "model_name": "o4-mini-2025-04-16",
  "webenv_path": "./webenv-init/webenv.py",
  "num_port": 30,
  "port_base": 8000
}
```

**Field Descriptions:**

* **input_type** â€” input source type, either `"html"` or `"url"`.
* **input_html_dir** â€” directory containing `.html` files (used when `input_type="html"`).
* **input_url_txt** â€” text file containing URLs (used when `input_type="url"`).
* **image_dir** â€” output directory for screenshots and logs.
* **bug_dir** â€” directory for saving abnormal or failed HTMLs/logs.
* **api_key** â€” model service key.
* **api_base** â€” model service endpoint (e.g., `https://api.openai.com/v1`).
* **model_name** â€” model name for inference.
* **webenv_path** â€” path to `webenv-init/webenv.py`, used to initialize browser environments.
* **num_port** â€” number of parallel ports for concurrent execution.
* **port_base** â€” base port number, actual range `[port_base, port_base + num_port - 1]`.

---

## WEBVIA â€” UI2Code Module

**Description**
The UI2Code subsystem processes exploration results (screenshots, task sequences, DOM logs) and uses multimodal language models to generate corresponding front-end HTML code.

**Minimal Example**

1. Enter the example directory:

```bash
cd example/ui2code
```

2. Run data preprocessing:

```bash
python process_agent_result.py --config config_process_data.json
```

3. Start UI2Code generation:

```bash
python start_ui2code.py --config config_ui2code.json
```

After execution:

* Processed UI data are saved in `input_data/`;
* Model outputs are saved as JSONL under `output/{model_name}_results.jsonl`;
* Rendered HTML files are saved under `output/{model_name}_html/`.


**Example Processed UI data**

```json
{
  "id": "19",
  "prompt": "Interaction 1ï¼šThis interaction involves multipule stepsâ€¦â€¦",
  "image_list": ["./images/.../start.png", "./images/.../_Input_..._Click.png"],
  "operation_info": [...]
}
```

**Fields:**

* **id** â€” unique identifier for each webpage or sample.
* **prompt** â€” description of all user interactions (e.g., input, click, select).

  * Each line like â€œInteraction 1â€¦â€ defines one operation sequence.
  * Please review or simplify this content manually before running.
* **image_list** â€” ordered list of all screenshots related to this sample.

  * The first is usually `start.png`; others reflect intermediate or result states.
* **operation_info** â€” structural details for later **Verification** module; you can ignore it when preparing data for UI2Code.

âœ… **Note:**
Only the `prompt` and `image_list` fields are used by the UI2Code model.
Ensure that the described interactions in `prompt` correspond to the images listed, and is what you desired. If not, manually delete any interaction parts with their images in the list.

**Example Configurations**

**(1) Data Preprocessing â€” `config_process_data.json`**

```json
{
  "input_folder": "../agent/images",
  "output_file": "./input_data/data_example.jsonl",
  "max_images": 20
}
```

**Fields:**

* **input_folder** â€” root directory from Agent outputs containing images and logs.
* **output_file** â€” path for the generated `.jsonl` file and the corresponding images.
* **max_images** â€” maximum allowed images per sample (default: 20).

**(2) Code Generation â€” `config_ui2code.json`**

```json
{
  "input_jsonl": "./input_data/data_example.jsonl",
  "output_prefix": "./output/",
  "models": [
    "claude-sonnet-4-20250514-thinking"
  ],
  "num_workers": 20,
  "api_base": "",
  "api_key": ""
}
```

**Fields:**

* **input_jsonl** â€” standardized input file path.
* **output_prefix** â€” output folder prefix (JSONL and HTML subfolders will be created automatically).
* **models** â€” model names to use for inference (supports multiple models).
* **num_workers** â€” number of concurrent workers (recommended â‰¤ CPU cores).
* **api_key / api_base** â€” credentials for API access.

**Example Output Structure**

```
example/UI2Code/
â”œâ”€â”€ config_process_data.json
â”œâ”€â”€ config_ui2code.json
â”œâ”€â”€ process_agent_result.py
â”œâ”€â”€ webvia-ui2code.py
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ gpt-5-2025-08-07_results.jsonl
â”‚   â””â”€â”€ gpt-5-2025-08-07_html/
â”‚       â”œâ”€â”€ 1001.html
â”‚       â”œâ”€â”€ 1002.html
â”‚       â””â”€â”€ ...
```

---

## WEBVIA â€” Verification Module

**Description**
The Verification subsystem validates the interactivity and correctness of HTML files generated by UI2Code.
It supports two modes:

* **Agent mode** â€” auto summarized tasks based on results from Agent module and UI2Code module.
* **Manual mode** â€” manually defined tasks.

Note: For real-world webpages of type URL, it is recommended to use Manual Mode to define tasks, since DOM elements on real websites often lack clear identifiers, making it difficult for the automated process to accurately recognize tasks.

**Minimal Example**

1. Enter the example directory:

```bash
cd example/verification
```

2. Prepare input data:

```bash
cp -r ../UI2Code/input_data .
cp -r ../UI2Code/{your_output_html_dir}/ ./htmls
eg. cp -r ../ui2code/example_output/claude-sonnet-4-20250514-thinking_html ./htmls
```

3. Launch verification (Agent mode):

```bash
python start_verify.py --config config_agent.json
```

**Example Agent mode task data**

Please refer to **Example Processed UI data**.
Program will automatically gernerate task from data.

**Example Manual mode task data**

```json
{
  "id": "19",
  "tasks": []
}
```


**Example Configuration (`config_agent.json`):**

```json
{
  "input_type": "agent",
  "input_dir": "./htmls",
  "task_source_jsonl": "./input_data/data_agent.jsonl",
  "output_image_dir": "./verify_images",
  "model_name": "gpt-5-2025-08-07",
  "bug_dir": "./buglogs",
  "api_base": "",
  "api_key": "",
  "num_port": 8,
  "port_base": 8000
}
```

**Fields:**

* **input_type** â€” task source type (`"agent"` or `"manual"`).
* **input_dir** â€” directory of HTML files to verify (usually UI2Code outputs).
* **task_source_jsonl** â€” input data reference (typically `input_data/*.jsonl`).
* **output_image_dir** â€” directory for screenshots and visual comparisons.
* **model_name** â€” model used for generating comparison descriptions and exploring webpages.
* **bug_dir** â€” directory for logs and error reports.
* **api_base / api_key** â€” API service endpoint and credentials.
* **num_port** â€” number of ports used for concurrency.
* **port_base** â€” base port number for parallel runs.

**Manual Mode Example**

```bash
python verify.py --config config_manual.json
```

When using Manual mode, set `"input_type": "manual"` and specify the path to `manual_samples/`. Please check `manual_samples/data_manual` for manual input example.

---

## ğŸ§  Reproducing Paper Experiments

To reproduce all experiments from the paper, execute the following modules sequentially:

**1. Agent Experiment: Pipeline**

```bash
cd evaluation/agent/pipeline
python start_agent_for_experiment.py
python rate_agent.py
```

**2. Agent Experiment: Single-Step-Action**

```bash
cd evaluation/agent/single-step-action
python call_actions.py
python rate_actions.py
```

**3. Agent Experiment: Single-Step-Compare**

```bash
cd evaluation/agent/single-step-compare
python call_compare.py
python rate_compare.py
```

**4. UI2Code Experiment**

Note: Because both the HTML generation and verification steps rely on large language models, this experiment exhibits high stochasticity, and results may fluctuate notably between runs. we release all raw experimental results used in the paper in the result_in_paper/ folder.

```bash
cd evaluation/ui2code
python ui2code_experiment.py --config config_ui2code.jsonl
python render_verify_all_model.py
python rate_verify.py
```

All results will be automatically saved in the respective `evaluation` subdirectories.

---

## ğŸ“˜ Citation

If you use this repository, code, or datasets in your research, please cite:

```
@article{xu2025webvia,
  title={WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation},
  author={Xu, Mingde and et al.},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025},
}
```
